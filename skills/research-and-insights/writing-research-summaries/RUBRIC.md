# Rubric

## Pass criteria
All must be true:

- [ ] Summary leads with most important finding, not methodology
- [ ] Each finding includes explicit "so what" implication for the audience
- [ ] Findings are prioritized (not presented as equally important)
- [ ] Confidence level or study limitations are stated
- [ ] Sample size and participant type are mentioned
- [ ] At least one clear recommendation or next step is included
- [ ] Language matches audience expertise level (no unexplained jargon)
- [ ] Summary fits the requested length constraint
- [ ] Recommendations link back to specific findings

## Fail criteria
Fail if any are true:

- [ ] Opens with methodology before findings
- [ ] Contains findings without implications ("users did X" with no "which means Y")
- [ ] Uses hedging language that obscures actionability ("might," "could potentially")
- [ ] Omits confidence indicators entirely
- [ ] Presents more than 7 key findings (dilutes focus)
- [ ] Includes raw data without interpretation
- [ ] Uses different terminology than the product uses
- [ ] Recommendations are generic ("do more research") vs. specific

## Audience-specific checks

### Executive summaries
- [ ] Fits on one page or screen
- [ ] Business impact is quantified or clearly described
- [ ] No methodology jargon (avoid "affinity mapping," "thematic analysis")

### Design summaries
- [ ] Includes direct user quotes
- [ ] Pain points have severity ratings
- [ ] Design directions are suggested (not just problems stated)

### Engineering summaries
- [ ] Technical areas are identified for each issue
- [ ] Performance thresholds are specific
- [ ] User mental models affecting architecture are explicit
